services:
  finetune:
    build: .
    image: llm-finetune
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - .:/app
      - hf_cache:/root/.cache/huggingface
    working_dir: /app
    env_file: .env
    environment: 
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - HF_HOME=/root/.cache/huggingface
      - TOKENIZERS_PARALLELISM=true
      - DATALOADER_WORKERS=0
      - NUM_EPOCHS=4.0
      - PER_DEVICE_TRAIN_BATCH_SIZE=4
      - GRAD_ACCUM_STEPS=1
      - MAX_LENGTH=256
      - LOAD_IN_4BIT=false
      - FP16=false   
      - BF16=true   
      - SAVE_STEPS=0 
      - LOGGING_STEPS=5
      - OPTIM=adamw_torch_fused 
      - BASE_MODEL=${BASE_MODEL:-mistralai/Mistral-7B-Instruct-v0.3}
    command: bash -c "python generate_modelfile.py && python finetune.py"
    profiles: ["finetune"]

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    volumes:
      - ollama_data:/root/.ollama
      - .:/app
    working_dir: /app
    healthcheck:
      test: ["CMD-SHELL", "ollama list > /dev/null 2>&1 || exit 1"]
      interval: 5s
      timeout: 3s
      retries: 30
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    profiles: ["ollama"]

  model-init:
    image: ollama/ollama:latest
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_HOST=ollama:11434
    volumes:
      - .:/app
    working_dir: /app
    entrypoint: ["/bin/sh", "-c"]
    command: "ollama show admissions-bot:latest >/dev/null 2>&1 || ollama create admissions-bot -f /app/Modelfile"
    profiles: ["ollama"]

volumes:
  ollama_data:
  hf_cache: